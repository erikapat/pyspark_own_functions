{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration properties of Apache Spark\n",
    "#sc.stop()\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "APP_NAME = 'pyspark_python'\n",
    "MASTER = 'local[*]'\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# load my own functions\n",
    "from utils.complete_missing_dates import *\n",
    "from utils.partitions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as psf\n",
    "from pyspark.sql import Window\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the data for the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use month partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (  # recreate the DataFrame\n",
    "    (1, datetime(2019, 12, 2, 14, 54, 17), 49.94),\n",
    "    (1, datetime(2019, 11, 3, 8, 58, 39), 50.49),\n",
    "    (1, datetime(2019, 8, 6, 10, 44, 1), 50.24),\n",
    "    (2, datetime(2019, 8, 2, 8, 58, 39), 62.32),\n",
    "    (2, datetime(2019, 5, 4, 10, 44, 1), 65.64))\n",
    "df = spark.createDataFrame(data, schema=(\"person\", \"timestamp\", \"weight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------+\n",
      "|person|          timestamp|weight|\n",
      "+------+-------------------+------+\n",
      "|     1|2019-12-02 14:54:17| 49.94|\n",
      "|     1|2019-11-03 08:58:39| 50.49|\n",
      "|     1|2019-08-06 10:44:01| 50.24|\n",
      "|     2|2019-08-02 08:58:39| 62.32|\n",
      "|     2|2019-05-04 10:44:01| 65.64|\n",
      "+------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- person: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "referece_col = 'person'\n",
    "time_col = 'timestamp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = complete_missing_months(df, time_col, referece_col, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort('person', 'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('partition_id', create_partitions_from_df('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------------+\n",
      "|          timestamp|person|weight|partition_id|\n",
      "+-------------------+------+------+------------+\n",
      "|2019-05-01 00:00:00|     1|  null|    20190531|\n",
      "|2019-06-01 00:00:00|     1|  null|    20190630|\n",
      "|2019-07-01 00:00:00|     1|  null|    20190731|\n",
      "|2019-08-01 00:00:00|     1| 50.24|    20190831|\n",
      "|2019-09-01 00:00:00|     1|  null|    20190930|\n",
      "|2019-10-01 00:00:00|     1|  null|    20191031|\n",
      "|2019-11-01 00:00:00|     1| 50.49|    20191130|\n",
      "|2019-12-01 00:00:00|     1| 49.94|    20191231|\n",
      "|2019-05-01 00:00:00|     2| 65.64|    20190531|\n",
      "|2019-06-01 00:00:00|     2|  null|    20190630|\n",
      "|2019-07-01 00:00:00|     2|  null|    20190731|\n",
      "|2019-08-01 00:00:00|     2| 62.32|    20190831|\n",
      "|2019-09-01 00:00:00|     2|  null|    20190930|\n",
      "|2019-10-01 00:00:00|     2|  null|    20191031|\n",
      "|2019-11-01 00:00:00|     2|  null|    20191130|\n",
      "|2019-12-01 00:00:00|     2|  null|    20191231|\n",
      "+-------------------+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('person', 'partition_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part = df.drop('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------+\n",
      "|person|weight|partition_id|\n",
      "+------+------+------------+\n",
      "|     1|  null|    20190531|\n",
      "|     1|  null|    20190630|\n",
      "|     1|  null|    20190731|\n",
      "|     1| 50.24|    20190831|\n",
      "|     1|  null|    20190930|\n",
      "|     1|  null|    20191031|\n",
      "|     1| 50.49|    20191130|\n",
      "|     1| 49.94|    20191231|\n",
      "|     2| 65.64|    20190531|\n",
      "|     2|  null|    20190630|\n",
      "|     2|  null|    20190731|\n",
      "|     2| 62.32|    20190831|\n",
      "|     2|  null|    20190930|\n",
      "|     2|  null|    20191031|\n",
      "|     2|  null|    20191130|\n",
      "|     2|  null|    20191231|\n",
      "+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_part.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "referece_col = 'person'\n",
    "time_col     = 'partition_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part = df_part.withColumn(time_col, montly_partition_YYmmdd(time_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- person: long (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- partition_id: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_part.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.date(2019, 5, 31),\n",
       " datetime.date(2019, 6, 30),\n",
       " datetime.date(2019, 7, 31),\n",
       " datetime.date(2019, 8, 31),\n",
       " datetime.date(2019, 9, 30),\n",
       " datetime.date(2019, 10, 31),\n",
       " datetime.date(2019, 11, 30),\n",
       " datetime.date(2019, 12, 31)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_intermediate_months(df_part, time_col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part = complete_missing_months(df_part, time_col, referece_col, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+\n",
      "|       partition_id|person|weight|\n",
      "+-------------------+------+------+\n",
      "|2019-05-01 00:00:00|     1|  null|\n",
      "|2019-06-01 00:00:00|     1|  null|\n",
      "|2019-07-01 00:00:00|     1|  null|\n",
      "|2019-08-01 00:00:00|     1| 50.24|\n",
      "|2019-09-01 00:00:00|     1|  null|\n",
      "|2019-10-01 00:00:00|     1|  null|\n",
      "|2019-11-01 00:00:00|     1| 50.49|\n",
      "|2019-12-01 00:00:00|     1| 49.94|\n",
      "|2019-05-01 00:00:00|     2| 65.64|\n",
      "|2019-06-01 00:00:00|     2|  null|\n",
      "|2019-07-01 00:00:00|     2|  null|\n",
      "|2019-08-01 00:00:00|     2| 62.32|\n",
      "|2019-09-01 00:00:00|     2|  null|\n",
      "|2019-10-01 00:00:00|     2|  null|\n",
      "|2019-11-01 00:00:00|     2|  null|\n",
      "|2019-12-01 00:00:00|     2|  null|\n",
      "+-------------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_part.sort('person', 'partition_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dates like integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (  # recreate the DataFrame\n",
    "    (1, 20191231, 49.94),\n",
    "    (1, 20191130, 50.49),\n",
    "    (1, 20191031, 50.24),\n",
    "    (1, 20190531, 55.24),\n",
    "    (2, 20190831, 62.32),\n",
    "    (2, 20190131, 65.64))\n",
    "df = spark.createDataFrame(data, schema=(\"person\", \"timestamp\", \"weight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"timestamp\", \n",
    "                             sf.date_format(sf.to_date(sf.unix_timestamp(df['timestamp'].cast('string'), \n",
    "                              \"yyyyMMdd\").cast(\"timestamp\")), 'yyyy-MM-dd'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n",
      "|person| timestamp|weight|\n",
      "+------+----------+------+\n",
      "|     1|2019-12-31| 49.94|\n",
      "|     1|2019-11-30| 50.49|\n",
      "|     1|2019-10-31| 50.24|\n",
      "|     1|2019-05-31| 55.24|\n",
      "|     2|2019-08-31| 62.32|\n",
      "|     2|2019-01-31| 65.64|\n",
      "+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "referece_col = 'person'\n",
    "time_col     = 'timestamp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = complete_missing_months(df, time_col, referece_col, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+\n",
      "|timestamp          |person|weight|\n",
      "+-------------------+------+------+\n",
      "|2019-01-01 00:00:00|1     |null  |\n",
      "|2019-02-01 00:00:00|1     |null  |\n",
      "|2019-03-01 00:00:00|1     |null  |\n",
      "|2019-04-01 00:00:00|1     |null  |\n",
      "|2019-05-01 00:00:00|1     |55.24 |\n",
      "|2019-06-01 00:00:00|1     |null  |\n",
      "|2019-07-01 00:00:00|1     |null  |\n",
      "|2019-08-01 00:00:00|1     |null  |\n",
      "|2019-09-01 00:00:00|1     |null  |\n",
      "|2019-10-01 00:00:00|1     |50.24 |\n",
      "|2019-11-01 00:00:00|1     |50.49 |\n",
      "|2019-12-01 00:00:00|1     |49.94 |\n",
      "|2019-01-01 00:00:00|2     |65.64 |\n",
      "|2019-02-01 00:00:00|2     |null  |\n",
      "|2019-03-01 00:00:00|2     |null  |\n",
      "|2019-04-01 00:00:00|2     |null  |\n",
      "|2019-05-01 00:00:00|2     |null  |\n",
      "|2019-06-01 00:00:00|2     |null  |\n",
      "|2019-07-01 00:00:00|2     |null  |\n",
      "|2019-08-01 00:00:00|2     |62.32 |\n",
      "|2019-09-01 00:00:00|2     |null  |\n",
      "|2019-10-01 00:00:00|2     |null  |\n",
      "|2019-11-01 00:00:00|2     |null  |\n",
      "|2019-12-01 00:00:00|2     |null  |\n",
      "+-------------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('person', 'timestamp').show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+\n",
      "|timestamp          |person|weight|\n",
      "+-------------------+------+------+\n",
      "|2019-01-01 00:00:00|1     |0.0   |\n",
      "|2019-02-01 00:00:00|1     |0.0   |\n",
      "|2019-03-01 00:00:00|1     |0.0   |\n",
      "|2019-04-01 00:00:00|1     |0.0   |\n",
      "|2019-05-01 00:00:00|1     |55.24 |\n",
      "|2019-06-01 00:00:00|1     |0.0   |\n",
      "|2019-07-01 00:00:00|1     |0.0   |\n",
      "|2019-08-01 00:00:00|1     |0.0   |\n",
      "|2019-09-01 00:00:00|1     |0.0   |\n",
      "|2019-10-01 00:00:00|1     |50.24 |\n",
      "|2019-11-01 00:00:00|1     |50.49 |\n",
      "|2019-12-01 00:00:00|1     |49.94 |\n",
      "|2019-01-01 00:00:00|2     |65.64 |\n",
      "|2019-02-01 00:00:00|2     |0.0   |\n",
      "|2019-03-01 00:00:00|2     |0.0   |\n",
      "|2019-04-01 00:00:00|2     |0.0   |\n",
      "|2019-05-01 00:00:00|2     |0.0   |\n",
      "|2019-06-01 00:00:00|2     |0.0   |\n",
      "|2019-07-01 00:00:00|2     |0.0   |\n",
      "|2019-08-01 00:00:00|2     |62.32 |\n",
      "|2019-09-01 00:00:00|2     |0.0   |\n",
      "|2019-10-01 00:00:00|2     |0.0   |\n",
      "|2019-11-01 00:00:00|2     |0.0   |\n",
      "|2019-12-01 00:00:00|2     |0.0   |\n",
      "+-------------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0).sort('person', 'timestamp').show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better complete with the following number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that we do not have values for the fisrt days, so we have to complete them with other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------+\n",
      "|person|          timestamp|weight|\n",
      "+------+-------------------+------+\n",
      "|     1|2019-01-01 00:00:00|  null|\n",
      "|     1|2019-02-01 00:00:00|  null|\n",
      "|     1|2019-03-01 00:00:00|  null|\n",
      "|     1|2019-04-01 00:00:00|  null|\n",
      "|     1|2019-05-01 00:00:00| 55.24|\n",
      "|     1|2019-06-01 00:00:00| 55.24|\n",
      "|     1|2019-07-01 00:00:00| 55.24|\n",
      "|     1|2019-08-01 00:00:00| 55.24|\n",
      "|     1|2019-09-01 00:00:00| 55.24|\n",
      "|     1|2019-10-01 00:00:00| 50.24|\n",
      "|     1|2019-11-01 00:00:00| 50.49|\n",
      "|     1|2019-12-01 00:00:00| 49.94|\n",
      "|     2|2019-01-01 00:00:00| 65.64|\n",
      "|     2|2019-02-01 00:00:00| 65.64|\n",
      "|     2|2019-03-01 00:00:00| 65.64|\n",
      "|     2|2019-04-01 00:00:00| 65.64|\n",
      "|     2|2019-05-01 00:00:00| 65.64|\n",
      "|     2|2019-06-01 00:00:00| 65.64|\n",
      "|     2|2019-07-01 00:00:00| 65.64|\n",
      "|     2|2019-08-01 00:00:00| 62.32|\n",
      "+------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_with_forward_value(df, 'person', 'weight', 'timestamp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------+\n",
      "|person|timestamp          |weight|\n",
      "+------+-------------------+------+\n",
      "|1     |2019-01-01 00:00:00|55.24 |\n",
      "|1     |2019-02-01 00:00:00|55.24 |\n",
      "|1     |2019-03-01 00:00:00|55.24 |\n",
      "|1     |2019-04-01 00:00:00|55.24 |\n",
      "|1     |2019-05-01 00:00:00|55.24 |\n",
      "|1     |2019-06-01 00:00:00|50.24 |\n",
      "|1     |2019-07-01 00:00:00|50.24 |\n",
      "|1     |2019-08-01 00:00:00|50.24 |\n",
      "|1     |2019-09-01 00:00:00|50.24 |\n",
      "|1     |2019-10-01 00:00:00|50.24 |\n",
      "|1     |2019-11-01 00:00:00|50.49 |\n",
      "|1     |2019-12-01 00:00:00|49.94 |\n",
      "|2     |2019-01-01 00:00:00|65.64 |\n",
      "|2     |2019-02-01 00:00:00|62.32 |\n",
      "|2     |2019-03-01 00:00:00|62.32 |\n",
      "|2     |2019-04-01 00:00:00|62.32 |\n",
      "|2     |2019-05-01 00:00:00|62.32 |\n",
      "|2     |2019-06-01 00:00:00|62.32 |\n",
      "|2     |2019-07-01 00:00:00|62.32 |\n",
      "|2     |2019-08-01 00:00:00|62.32 |\n",
      "|2     |2019-09-01 00:00:00|null  |\n",
      "|2     |2019-10-01 00:00:00|null  |\n",
      "|2     |2019-11-01 00:00:00|null  |\n",
      "|2     |2019-12-01 00:00:00|null  |\n",
      "+------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_with_backward_value(df, 'person', 'weight', 'timestamp').show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------+--------------+--------------------+\n",
      "|person|          timestamp|weight|prev_day_price|        daily_return|\n",
      "+------+-------------------+------+--------------+--------------------+\n",
      "|     1|2019-01-01 00:00:00| 55.24|          null|                null|\n",
      "|     1|2019-02-01 00:00:00| 55.24|         55.24|                 0.0|\n",
      "|     1|2019-03-01 00:00:00| 55.24|         55.24|                 0.0|\n",
      "|     1|2019-04-01 00:00:00| 55.24|         55.24|                 0.0|\n",
      "|     1|2019-05-01 00:00:00| 55.24|         55.24|                 0.0|\n",
      "|     1|2019-06-01 00:00:00| 50.24|         55.24|-0.09952229299363057|\n",
      "|     1|2019-07-01 00:00:00| 50.24|         50.24|                 0.0|\n",
      "|     1|2019-08-01 00:00:00| 50.24|         50.24|                 0.0|\n",
      "|     1|2019-09-01 00:00:00| 50.24|         50.24|                 0.0|\n",
      "|     1|2019-10-01 00:00:00| 50.24|         50.24|                 0.0|\n",
      "|     1|2019-11-01 00:00:00| 50.49|         50.24|0.004951475539710834|\n",
      "|     1|2019-12-01 00:00:00| 49.94|         50.49|-0.01101321585903...|\n",
      "|     2|2019-01-01 00:00:00| 65.64|          null|                null|\n",
      "|     2|2019-02-01 00:00:00| 62.32|         65.64|-0.05327342747111682|\n",
      "|     2|2019-03-01 00:00:00| 62.32|         62.32|                 0.0|\n",
      "|     2|2019-04-01 00:00:00| 62.32|         62.32|                 0.0|\n",
      "|     2|2019-05-01 00:00:00| 62.32|         62.32|                 0.0|\n",
      "|     2|2019-06-01 00:00:00| 62.32|         62.32|                 0.0|\n",
      "|     2|2019-07-01 00:00:00| 62.32|         62.32|                 0.0|\n",
      "|     2|2019-08-01 00:00:00| 62.32|         62.32|                 0.0|\n",
      "+------+-------------------+------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "df = complete_with_backward_value(df, 'person', 'weight', 'timestamp')\n",
    "### Defining the window \n",
    "Windowspec=Window.partitionBy(\"person\").orderBy(\"timestamp\")\n",
    "\n",
    "### Calculating lag of price at each day level\n",
    "prev_day_price= df.withColumn('prev_day_price',\n",
    "                        func.lag(df['weight'])\n",
    "                                .over(Windowspec))\n",
    "\n",
    "result = prev_day_price.withColumn('daily_return', \n",
    "          (prev_day_price['weight'] - prev_day_price['prev_day_price']) / \n",
    "prev_day_price['weight'] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"grouping expressions sequence is empty, and '`person`' is not an aggregate function. Wrap '(sum(CAST(`add` AS BIGINT)) AS `max`)' in windowing function(s) or wrap '`person`' in first() (or first_value) if you don't care which value you get.;;\\nAggregate [person#171L, timestamp#5986, weight#5987, add#25522, sum(cast(add#25522 as bigint)) AS max#25528L]\\n+- Project [person#171L, timestamp#5986, weight#5987, -1 AS add#25522]\\n   +- Project [person#171L, coalesce(timestamp#3535, to_timestamp('timestamp, None)) AS timestamp#5986, coalesce(weight#3536, completed_column#5981) AS weight#5987]\\n      +- Project [person#171L, timestamp#3535, weight#3536, completed_column#5981]\\n         +- Project [person#171L, timestamp#3535, weight#3536, completed_column#5981, completed_column#5981]\\n            +- Window [first(weight#3536, true) windowspecdefinition(person#171L, timestamp#3535 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), unboundedfollowing$())) AS completed_column#5981], [person#171L], [timestamp#3535 ASC NULLS FIRST]\\n               +- Project [person#171L, timestamp#3535, weight#3536]\\n                  +- Project [person#171L, coalesce(timestamp#1485, to_timestamp('timestamp, None)) AS timestamp#3535, coalesce(weight#1486, completed_column#3530) AS weight#3536]\\n                     +- Project [person#171L, timestamp#1485, weight#1486, completed_column#3530]\\n                        +- Project [person#171L, timestamp#1485, weight#1486, completed_column#3530, completed_column#3530]\\n                           +- Window [first(weight#1486, true) windowspecdefinition(person#171L, timestamp#1485 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), unboundedfollowing$())) AS completed_column#3530], [person#171L], [timestamp#1485 ASC NULLS FIRST]\\n                              +- Project [person#171L, timestamp#1485, weight#1486]\\n                                 +- Project [person#171L, coalesce(timestamp#1474, to_timestamp('timestamp, None)) AS timestamp#1485, coalesce(weight#1475, completed_column#1480) AS weight#1486]\\n                                    +- Project [person#171L, timestamp#1474, weight#1475, completed_column#1480]\\n                                       +- Project [person#171L, timestamp#1474, weight#1475, completed_column#1480, completed_column#1480]\\n                                          +- Window [first(weight#1475, true) windowspecdefinition(person#171L, timestamp#1474 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), unboundedfollowing$())) AS completed_column#1480], [person#171L], [timestamp#1474 ASC NULLS FIRST]\\n                                             +- Project [person#171L, timestamp#1474, weight#1475]\\n                                                +- Project [person#171L, coalesce(timestamp#170, to_timestamp('timestamp, None)) AS timestamp#1474, coalesce(weight#130, completed_column#1469) AS weight#1475]\\n                                                   +- Project [timestamp#170, person#171L, weight#130, completed_column#1469]\\n                                                      +- Project [timestamp#170, person#171L, weight#130, completed_column#1469, completed_column#1469]\\n                                                         +- Window [first(weight#130, true) windowspecdefinition(person#171L, timestamp#170 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), unboundedfollowing$())) AS completed_column#1469], [person#171L], [timestamp#170 ASC NULLS FIRST]\\n                                                            +- Project [timestamp#170, person#171L, weight#130]\\n                                                               +- Project [coalesce(timestamp#148, cast(timestamp#167 as timestamp)) AS timestamp#170, coalesce(person#128L, person#166L) AS person#171L, weight#130]\\n                                                                  +- Join FullOuter, ((timestamp#148 = cast(timestamp#167 as timestamp)) && (person#128L = person#166L))\\n                                                                     :- Project [person#128L, cast(unix_timestamp(trunc(cast(timestamp#134 as date), month), yyyy-MM-dd HH:mm:ss, Some(Europe/Madrid)) as timestamp) AS timestamp#148, weight#130]\\n                                                                     :  +- Project [person#128L, date_format(cast(to_date(cast(unix_timestamp(cast(timestamp#129L as string), yyyyMMdd, None) as timestamp), None) as timestamp), yyyy-MM-dd, Some(Europe/Madrid)) AS timestamp#134, weight#130]\\n                                                                     :     +- LogicalRDD [person#128L, timestamp#129L, weight#130], false\\n                                                                     +- LogicalRDD [person#166L, timestamp#167], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1171.withColumn.\n: org.apache.spark.sql.AnalysisException: grouping expressions sequence is empty, and '`person`' is not an aggregate function. Wrap '(sum(CAST(`add` AS BIGINT)) AS `max`)' in windowing function(s) or wrap '`person`' in first() (or first_value) if you don't care which value you get.;;\nAggregate [person#171L, timestamp#5986, weight#5987, add#25522, sum(cast(add#25522 as bigint)) AS max#25528L]\n+- Project [person#171L, timestamp#5986, weight#5987, -1 AS add#25522]\n   +- Project [person#171L, coalesce(timestamp#3535, to_timestamp('timestamp, None)) AS timestamp#5986, coalesce(weight#3536, completed_column#5981) AS weight#5987]\n      +- Project [person#171L, timestamp#3535, weight#3536, completed_column#5981]\n         +- Project [person#171L, timestamp#3535, weight#3536, completed_column#5981, completed_column#5981]\n            +- Window [first(weight#3536, true) windowspecdefinition(person#171L, timestamp#3535 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), unboundedfollowing$())) AS completed_column#5981], [person#171L], [timestamp#3535 ASC NULLS FIRST]\n               +- Project [person#171L, timestamp#3535, weight#3536]\n                  +- Project [person#171L, coalesce(timestamp#1485, to_timestamp('timestamp, None)) AS timestamp#3535, coalesce(weight#1486, completed_column#3530) AS weight#3536]\n                     +- Project [person#171L, timestamp#1485, weight#1486, completed_column#3530]\n                        +- Project [person#171L, timestamp#1485, weight#1486, completed_column#3530, completed_column#3530]\n                           +- Window [first(weight#1486, true) windowspecdefinition(person#171L, timestamp#1485 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), unboundedfollowing$())) AS completed_column#3530], [person#171L], [timestamp#1485 ASC NULLS FIRST]\n                              +- Project [person#171L, timestamp#1485, weight#1486]\n                                 +- Project [person#171L, coalesce(timestamp#1474, to_timestamp('timestamp, None)) AS timestamp#1485, coalesce(weight#1475, completed_column#1480) AS weight#1486]\n                                    +- Project [person#171L, timestamp#1474, weight#1475, completed_column#1480]\n                                       +- Project [person#171L, timestamp#1474, weight#1475, completed_column#1480, completed_column#1480]\n                                          +- Window [first(weight#1475, true) windowspecdefinition(person#171L, timestamp#1474 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), unboundedfollowing$())) AS completed_column#1480], [person#171L], [timestamp#1474 ASC NULLS FIRST]\n                                             +- Project [person#171L, timestamp#1474, weight#1475]\n                                                +- Project [person#171L, coalesce(timestamp#170, to_timestamp('timestamp, None)) AS timestamp#1474, coalesce(weight#130, completed_column#1469) AS weight#1475]\n                                                   +- Project [timestamp#170, person#171L, weight#130, completed_column#1469]\n                                                      +- Project [timestamp#170, person#171L, weight#130, completed_column#1469, completed_column#1469]\n                                                         +- Window [first(weight#130, true) windowspecdefinition(person#171L, timestamp#170 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), unboundedfollowing$())) AS completed_column#1469], [person#171L], [timestamp#170 ASC NULLS FIRST]\n                                                            +- Project [timestamp#170, person#171L, weight#130]\n                                                               +- Project [coalesce(timestamp#148, cast(timestamp#167 as timestamp)) AS timestamp#170, coalesce(person#128L, person#166L) AS person#171L, weight#130]\n                                                                  +- Join FullOuter, ((timestamp#148 = cast(timestamp#167 as timestamp)) && (person#128L = person#166L))\n                                                                     :- Project [person#128L, cast(unix_timestamp(trunc(cast(timestamp#134 as date), month), yyyy-MM-dd HH:mm:ss, Some(Europe/Madrid)) as timestamp) AS timestamp#148, weight#130]\n                                                                     :  +- Project [person#128L, date_format(cast(to_date(cast(unix_timestamp(cast(timestamp#129L as string), yyyyMMdd, None) as timestamp), None) as timestamp), yyyy-MM-dd, Some(Europe/Madrid)) AS timestamp#134, weight#130]\n                                                                     :     +- LogicalRDD [person#128L, timestamp#129L, weight#130], false\n                                                                     +- LogicalRDD [person#166L, timestamp#167], false\n\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:43)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:217)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:258)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:258)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:258)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2258)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2225)\n\tat sun.reflect.GeneratedMethodAccessor148.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a3aa0c762201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'add'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'add'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m cum_sum =cum_sum.withColumn('cumsum', f.sum(col('add')).over(Window.partitionBy(col('person'))\n\u001b[1;32m      6\u001b[0m                                                                           .orderBy().rowsBetween(-sys.maxsize, 0)))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \"\"\"\n\u001b[1;32m   1997\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"grouping expressions sequence is empty, and '`person`' is not an aggregate function. Wrap '(sum(CAST(`add` AS BIGINT)) AS `max`)' in windowing function(s) or wrap '`person`' in first() (or first_value) if you don't care which value you get.;;\\nAggregate [person#171L, timestamp#5986, weight#5987, add#25522, sum(cast(add#25522 as bigint)) AS max#25528L]\\n+- Project [person#171L, timestamp#5986, weight#5987, -1 AS add#25522]\\n   +- Project [person#171L, coalesce(timestamp#3535, to_timestamp('timestamp, None)) AS timestamp#5986, coalesce(weight#3536, completed_column#5981) AS weight#5987]\\n      +- Project [person#171L, timestamp#3535, weight#3536, completed_column#5981]\\n         +- Project [person#171L, timestamp#3535, weight#3536, completed_column#5981, completed_column#5981]\\n            +- Window [first(weight#3536, true) windowspecdefinition(person#171L, timestamp#3535 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), unboundedfollowing$())) AS completed_column#5981], [person#171L], [timestamp#3535 ASC NULLS FIRST]\\n               +- Project [person#171L, timestamp#3535, weight#3536]\\n                  +- Project [person#171L, coalesce(timestamp#1485, to_timestamp('timestamp, None)) AS timestamp#3535, coalesce(weight#1486, completed_column#3530) AS weight#3536]\\n                     +- Project [person#171L, timestamp#1485, weight#1486, completed_column#3530]\\n                        +- Project [person#171L, timestamp#1485, weight#1486, completed_column#3530, completed_column#3530]\\n                           +- Window [first(weight#1486, true) windowspecdefinition(person#171L, timestamp#1485 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), unboundedfollowing$())) AS completed_column#3530], [person#171L], [timestamp#1485 ASC NULLS FIRST]\\n                              +- Project [person#171L, timestamp#1485, weight#1486]\\n                                 +- Project [person#171L, coalesce(timestamp#1474, to_timestamp('timestamp, None)) AS timestamp#1485, coalesce(weight#1475, completed_column#1480) AS weight#1486]\\n                                    +- Project [person#171L, timestamp#1474, weight#1475, completed_column#1480]\\n                                       +- Project [person#171L, timestamp#1474, weight#1475, completed_column#1480, completed_column#1480]\\n                                          +- Window [first(weight#1475, true) windowspecdefinition(person#171L, timestamp#1474 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), unboundedfollowing$())) AS completed_column#1480], [person#171L], [timestamp#1474 ASC NULLS FIRST]\\n                                             +- Project [person#171L, timestamp#1474, weight#1475]\\n                                                +- Project [person#171L, coalesce(timestamp#170, to_timestamp('timestamp, None)) AS timestamp#1474, coalesce(weight#130, completed_column#1469) AS weight#1475]\\n                                                   +- Project [timestamp#170, person#171L, weight#130, completed_column#1469]\\n                                                      +- Project [timestamp#170, person#171L, weight#130, completed_column#1469, completed_column#1469]\\n                                                         +- Window [first(weight#130, true) windowspecdefinition(person#171L, timestamp#170 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), unboundedfollowing$())) AS completed_column#1469], [person#171L], [timestamp#170 ASC NULLS FIRST]\\n                                                            +- Project [timestamp#170, person#171L, weight#130]\\n                                                               +- Project [coalesce(timestamp#148, cast(timestamp#167 as timestamp)) AS timestamp#170, coalesce(person#128L, person#166L) AS person#171L, weight#130]\\n                                                                  +- Join FullOuter, ((timestamp#148 = cast(timestamp#167 as timestamp)) && (person#128L = person#166L))\\n                                                                     :- Project [person#128L, cast(unix_timestamp(trunc(cast(timestamp#134 as date), month), yyyy-MM-dd HH:mm:ss, Some(Europe/Madrid)) as timestamp) AS timestamp#148, weight#130]\\n                                                                     :  +- Project [person#128L, date_format(cast(to_date(cast(unix_timestamp(cast(timestamp#129L as string), yyyyMMdd, None) as timestamp), None) as timestamp), yyyy-MM-dd, Some(Europe/Madrid)) AS timestamp#134, weight#130]\\n                                                                     :     +- LogicalRDD [person#128L, timestamp#129L, weight#130], false\\n                                                                     +- LogicalRDD [person#166L, timestamp#167], false\\n\""
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "df = df.withColumn('add', lit(-1)).withColumn('max', lit(f.sum(col('add'))))\n",
    "cum_sum =cum_sum.withColumn('cumsum', f.sum(col('add')).over(Window.partitionBy(col('person'))\n",
    "                                                                          .orderBy().rowsBetween(-sys.maxsize, 0)))\n",
    "cum_sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
