{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration properties of Apache Spark\n",
    "#sc.stop()\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "APP_NAME = 'pyspark_python'\n",
    "MASTER = 'local[*]'\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as psf\n",
    "from pyspark.sql import Window\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (  # recreate the DataFrame\n",
    "    (1, datetime(2019, 12, 2, 14, 54, 17), 49.94),\n",
    "    (1, datetime(2019, 12, 3, 8, 58, 39), 50.49),\n",
    "    (1, datetime(2019, 12, 6, 10, 44, 1), 50.24),\n",
    "    (2, datetime(2019, 12, 2, 8, 58, 39), 62.32),\n",
    "    (2, datetime(2019, 12, 4, 10, 44, 1), 65.64))\n",
    "df = spark.createDataFrame(data, schema=(\"person\", \"timestamp\", \"weight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50.24]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.approxQuantile(\"weight\", [0.5], 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[50.24], [1.0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.approxQuantile([\"weight\", \"person\"], [0.5], 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/31432843/how-to-find-median-and-quantiles-using-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom pyspark.sql.functions import col\\nfrom pyspark.sql.functions import split\\n \\n(df.withColumn(\"col1\", split(col(\"letters\"), \"-\").getItem(0))\\n.withColumn(\"col2\", split(col(\"letters\"), \"-\").getItem(1)).show())\\n\\nfrom pyspark.sql.functions import split\\n \\n(df.withColumn(\"col1\", split(col(\"letters\"), \"123\").getItem(0))\\n.withColumn(\"col2\", split(col(\"letters\"), \"123\").getItem(1)).show())\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import split\n",
    " \n",
    "(df.withColumn(\"col1\", split(col(\"letters\"), \"-\").getItem(0))\n",
    ".withColumn(\"col2\", split(col(\"letters\"), \"-\").getItem(1)).show())\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    " \n",
    "(df.withColumn(\"col1\", split(col(\"letters\"), \"123\").getItem(0))\n",
    ".withColumn(\"col2\", split(col(\"letters\"), \"123\").getItem(1)).show())\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-67f466c09265>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-67f466c09265>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import spark_summit spark_summit\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import spark_summit spark_summit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-68422e41b1ad>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-68422e41b1ad>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ./bin/spark-submit spark_summit.py\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "-----------------------------------------------------------------------------------------------------\n",
    "from pyspark.sql.functions import length\n",
    "def contract_values(df, contract_number, contract_name):\n",
    "    df = (df\n",
    "    .withColumn('leng', length(col(contract_number)))\n",
    "    .withColumn('leng9', length(col(contract_number)) - 9 )\n",
    "    .withColumn('leng19', length(col(contract_number)) - (9 + 26) )\n",
    "    .withColumn(contract_name, sf.col(contract_number)\n",
    "                .substr(length(col(contract_number)) - (9 + 26),  length(col(contract_number)) - 18  )) #14\n",
    "    .withColumn('leng_final', length(col(contract_name)))\n",
    "\n",
    "          )\n",
    "    return df# run the code as a terminal command\n",
    "./bin/spark-submit spark_summit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
