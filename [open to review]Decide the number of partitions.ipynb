{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioner\n",
    "\n",
    "- When processing, Spark assigns one task for each partition and each worker threads can only process one task at a time. Thus, with too few partitions, the application won’t utilize all the cores available in the cluster and it can cause data skewing problem; with too many partitions, it will bring overhead for Spark to manage too many small tasks.\n",
    "\n",
    "- Partitioner class is used to partition data based on keys. It accepts two parameters numPartitions and partitionFunc to initiate as the following code shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.rdd import portable_hash\n",
    "from pyspark import Row\n",
    "\n",
    "appName = \"PySpark Partition Example\"\n",
    "master = \"local[8]\"\n",
    "\n",
    "# Create Spark session with Hive supported.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()\n",
    "print(spark.version)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12 records populated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+\n",
      "|Amount|Country| ID|\n",
      "+------+-------+---+\n",
      "|    11|     AU|  1|\n",
      "|    12|     US|  2|\n",
      "|    13|     CN|  3|\n",
      "|    14|     AU|  4|\n",
      "|    15|     US|  5|\n",
      "|    16|     CN|  6|\n",
      "|    17|     AU|  7|\n",
      "|    18|     US|  8|\n",
      "|    19|     CN|  9|\n",
      "|    20|     AU| 10|\n",
      "|    21|     US| 11|\n",
      "|    22|     CN| 12|\n",
      "+------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Populate sample data\n",
    "countries = (\"CN\", \"AU\", \"US\")\n",
    "data = []\n",
    "for i in range(1, 13):\n",
    "    data.append({\"ID\": i, \"Country\": countries[i % 3],  \"Amount\": 10+i})\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from print_partitions function is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_partitions(df):\n",
    "    numPartitions = df.rdd.getNumPartitions()\n",
    "    print(\"Total partitions: {}\".format(numPartitions))\n",
    "    print(\"Partitioner: {}\".format(df.rdd.partitioner))\n",
    "    df.explain()\n",
    "    parts = df.rdd.glom().collect()\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for p in parts:\n",
    "        print(\"Partition {}:\".format(i))\n",
    "        for r in p:\n",
    "            print(\"Row {}:{}\".format(j, r))\n",
    "            j = j+1\n",
    "        i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 8\n",
      "Partitioner: None\n",
      "== Physical Plan ==\n",
      "Scan ExistingRDD[Amount#0L,Country#1,ID#2L]\n",
      "Partition 0:\n",
      "Row 0:Row(Amount=11, Country='AU', ID=1)\n",
      "Partition 1:\n",
      "Row 1:Row(Amount=12, Country='US', ID=2)\n",
      "Row 2:Row(Amount=13, Country='CN', ID=3)\n",
      "Partition 2:\n",
      "Row 3:Row(Amount=14, Country='AU', ID=4)\n",
      "Partition 3:\n",
      "Row 4:Row(Amount=15, Country='US', ID=5)\n",
      "Row 5:Row(Amount=16, Country='CN', ID=6)\n",
      "Partition 4:\n",
      "Row 6:Row(Amount=17, Country='AU', ID=7)\n",
      "Partition 5:\n",
      "Row 7:Row(Amount=18, Country='US', ID=8)\n",
      "Row 8:Row(Amount=19, Country='CN', ID=9)\n",
      "Partition 6:\n",
      "Row 9:Row(Amount=20, Country='AU', ID=10)\n",
      "Partition 7:\n",
      "Row 10:Row(Amount=21, Country='US', ID=11)\n",
      "Row 11:Row(Amount=22, Country='CN', ID=12)\n"
     ]
    }
   ],
   "source": [
    "print_partitions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition data\n",
    "Let’s repartition the data to three partitions only by Country column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 3\n",
      "Partitioner: None\n",
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(Country#1, 3)\n",
      "+- Scan ExistingRDD[Amount#0L,Country#1,ID#2L]\n",
      "Partition 0:\n",
      "Partition 1:\n",
      "Row 0:Row(Amount=12, Country='US', ID=2)\n",
      "Row 1:Row(Amount=13, Country='CN', ID=3)\n",
      "Row 2:Row(Amount=15, Country='US', ID=5)\n",
      "Row 3:Row(Amount=16, Country='CN', ID=6)\n",
      "Row 4:Row(Amount=18, Country='US', ID=8)\n",
      "Row 5:Row(Amount=19, Country='CN', ID=9)\n",
      "Row 6:Row(Amount=21, Country='US', ID=11)\n",
      "Row 7:Row(Amount=22, Country='CN', ID=12)\n",
      "Partition 2:\n",
      "Row 8:Row(Amount=11, Country='AU', ID=1)\n",
      "Row 9:Row(Amount=14, Country='AU', ID=4)\n",
      "Row 10:Row(Amount=17, Country='AU', ID=7)\n",
      "Row 11:Row(Amount=20, Country='AU', ID=10)\n"
     ]
    }
   ],
   "source": [
    "numPartitions = 3\n",
    "\n",
    "df = df.repartition(numPartitions, \"Country\")\n",
    "\n",
    "print_partitions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may expect that each partition includes data for each Country but that is not the case. Why? Because repartition function by default uses hash partitioning. For different country code, it may be allocated into the same partition number.\n",
    "\n",
    "We can verify this by using the following code to calculate the hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------------------+----------+\n",
      "|Amount|Country| ID|               Hash#|Partition#|\n",
      "+------+-------+---+--------------------+----------+\n",
      "|    12|     US|  2|-8328537658613580243|      -1.0|\n",
      "|    13|     CN|  3|-7458853143580063552|      -1.0|\n",
      "|    15|     US|  5|-8328537658613580243|      -1.0|\n",
      "|    16|     CN|  6|-7458853143580063552|      -1.0|\n",
      "|    18|     US|  8|-8328537658613580243|      -1.0|\n",
      "|    19|     CN|  9|-7458853143580063552|      -1.0|\n",
      "|    21|     US| 11|-8328537658613580243|      -1.0|\n",
      "|    22|     CN| 12|-7458853143580063552|      -1.0|\n",
      "|    11|     AU|  1| 6593628092971972691|       0.0|\n",
      "|    14|     AU|  4| 6593628092971972691|       0.0|\n",
      "|    17|     AU|  7| 6593628092971972691|       0.0|\n",
      "|    20|     AU| 10| 6593628092971972691|       0.0|\n",
      "+------+-------+---+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_portable_hash = udf(lambda str: portable_hash(str))\n",
    "\n",
    "df = df.withColumn(\"Hash#\", udf_portable_hash(df.Country))\n",
    "\n",
    "df = df.withColumn(\"Partition#\", df[\"Hash#\"] % numPartitions)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is consistent with the previous one as record ID 1,4,7,10 are allocated to one partition while the others are allocated to another question. There is also one partition with empty content as no records are allocated to that partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate one partition for each key value\n",
    "For the above example, if we want to allocate one partition for each Country (CN, US, AU), what should we do?\n",
    "\n",
    "Well, the first thing we can try is to increase the partition number. In this way, the chance for allocating each different value to different partition is higher.\n",
    "\n",
    "So if we increate the partition number to 5.\n",
    "\n",
    "The output shows that each country’s data is now located in the same partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 5\n",
      "Partitioner: None\n",
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(Country#1, 5)\n",
      "+- *(1) Project [Amount#0L, Country#1, ID#2L, pythonUDF1#47 AS Hash##17, (cast(pythonUDF1#47 as double) % 3.0) AS Partition##22]\n",
      "   +- BatchEvalPython [<lambda>(Country#1), <lambda>(Country#1)], [Amount#0L, Country#1, ID#2L, pythonUDF0#46, pythonUDF1#47]\n",
      "      +- Exchange hashpartitioning(Country#1, 3)\n",
      "         +- Scan ExistingRDD[Amount#0L,Country#1,ID#2L]\n",
      "Partition 0:\n",
      "Partition 1:\n",
      "Partition 2:\n",
      "Row 0:Row(Amount=12, Country='US', ID=2, Hash#='-8328537658613580243', Partition#=-1.0)\n",
      "Row 1:Row(Amount=15, Country='US', ID=5, Hash#='-8328537658613580243', Partition#=-1.0)\n",
      "Row 2:Row(Amount=18, Country='US', ID=8, Hash#='-8328537658613580243', Partition#=-1.0)\n",
      "Row 3:Row(Amount=21, Country='US', ID=11, Hash#='-8328537658613580243', Partition#=-1.0)\n",
      "Partition 3:\n",
      "Row 4:Row(Amount=13, Country='CN', ID=3, Hash#='-7458853143580063552', Partition#=-1.0)\n",
      "Row 5:Row(Amount=16, Country='CN', ID=6, Hash#='-7458853143580063552', Partition#=-1.0)\n",
      "Row 6:Row(Amount=19, Country='CN', ID=9, Hash#='-7458853143580063552', Partition#=-1.0)\n",
      "Row 7:Row(Amount=22, Country='CN', ID=12, Hash#='-7458853143580063552', Partition#=-1.0)\n",
      "Partition 4:\n",
      "Row 8:Row(Amount=11, Country='AU', ID=1, Hash#='6593628092971972691', Partition#=0.0)\n",
      "Row 9:Row(Amount=14, Country='AU', ID=4, Hash#='6593628092971972691', Partition#=0.0)\n",
      "Row 10:Row(Amount=17, Country='AU', ID=7, Hash#='6593628092971972691', Partition#=0.0)\n",
      "Row 11:Row(Amount=20, Country='AU', ID=10, Hash#='6593628092971972691', Partition#=0.0)\n",
      "+------+-------+---+--------------------+----------+\n",
      "|Amount|Country| ID|               Hash#|Partition#|\n",
      "+------+-------+---+--------------------+----------+\n",
      "|    12|     US|  2|-8328537658613580243|      -1.0|\n",
      "|    15|     US|  5|-8328537658613580243|      -1.0|\n",
      "|    18|     US|  8|-8328537658613580243|      -1.0|\n",
      "|    21|     US| 11|-8328537658613580243|      -1.0|\n",
      "|    13|     CN|  3|-7458853143580063552|      -4.0|\n",
      "|    16|     CN|  6|-7458853143580063552|      -4.0|\n",
      "|    19|     CN|  9|-7458853143580063552|      -4.0|\n",
      "|    22|     CN| 12|-7458853143580063552|      -4.0|\n",
      "|    11|     AU|  1| 6593628092971972691|       3.0|\n",
      "|    14|     AU|  4| 6593628092971972691|       3.0|\n",
      "|    17|     AU|  7| 6593628092971972691|       3.0|\n",
      "|    20|     AU| 10| 6593628092971972691|       3.0|\n",
      "+------+-------+---+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numPartitions = 5\n",
    "\n",
    "df = df.repartition(numPartitions, \"Country\")\n",
    "\n",
    "print_partitions(df)\n",
    "\n",
    "udf_portable_hash = udf(lambda str: portable_hash(str))\n",
    "\n",
    "df = df.withColumn(\"Hash#\", udf_portable_hash(df.Country))\n",
    "\n",
    "df = df.withColumn(\"Partition#\", df[\"Hash#\"] % numPartitions)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what if the hashing algorithm generates the same hash code/number?\n",
    "\n",
    "#### Use partitionBy function\n",
    "To address the above issue, we can create a customised partitioning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_partitioning(k):\n",
    "    return countries.index(k)\n",
    "    \n",
    "udf_country_hash = udf(lambda str: country_partitioning(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 3\n",
      "Partitioner: None\n",
      "== Physical Plan ==\n",
      "Scan ExistingRDD[_1#19,_2#20]\n",
      "Partition 0:\n",
      "Row 0:Row(_1='CN', _2=Row(Amount=13, Country='CN', ID=3))\n",
      "Row 1:Row(_1='CN', _2=Row(Amount=16, Country='CN', ID=6))\n",
      "Row 2:Row(_1='CN', _2=Row(Amount=19, Country='CN', ID=9))\n",
      "Row 3:Row(_1='CN', _2=Row(Amount=22, Country='CN', ID=12))\n",
      "Partition 1:\n",
      "Row 4:Row(_1='AU', _2=Row(Amount=11, Country='AU', ID=1))\n",
      "Row 5:Row(_1='AU', _2=Row(Amount=14, Country='AU', ID=4))\n",
      "Row 6:Row(_1='AU', _2=Row(Amount=17, Country='AU', ID=7))\n",
      "Row 7:Row(_1='AU', _2=Row(Amount=20, Country='AU', ID=10))\n",
      "Partition 2:\n",
      "Row 8:Row(_1='US', _2=Row(Amount=12, Country='US', ID=2))\n",
      "Row 9:Row(_1='US', _2=Row(Amount=15, Country='US', ID=5))\n",
      "Row 10:Row(_1='US', _2=Row(Amount=18, Country='US', ID=8))\n",
      "Row 11:Row(_1='US', _2=Row(Amount=21, Country='US', ID=11))\n",
      "+---+------------+-----+----------+\n",
      "|_1 |_2          |Hash#|Partition#|\n",
      "+---+------------+-----+----------+\n",
      "|CN |[13, CN, 3] |0    |0.0       |\n",
      "|CN |[16, CN, 6] |0    |0.0       |\n",
      "|CN |[19, CN, 9] |0    |0.0       |\n",
      "|CN |[22, CN, 12]|0    |0.0       |\n",
      "|AU |[11, AU, 1] |1    |1.0       |\n",
      "|AU |[14, AU, 4] |1    |1.0       |\n",
      "|AU |[17, AU, 7] |1    |1.0       |\n",
      "|AU |[20, AU, 10]|1    |1.0       |\n",
      "|US |[12, US, 2] |2    |2.0       |\n",
      "|US |[15, US, 5] |2    |2.0       |\n",
      "|US |[18, US, 8] |2    |2.0       |\n",
      "|US |[21, US, 11]|2    |2.0       |\n",
      "+---+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.rdd \\\n",
    "    .map(lambda el: (el[\"Country\"], el)) \\\n",
    "    .partitionBy(numPartitions, country_partitioning) \\\n",
    "    .toDF()\n",
    "print_partitions(df)\n",
    "\n",
    "df = df.withColumn(\"Hash#\", udf_country_hash(df[0]))\n",
    "df = df.withColumn(\"Partition#\", df[\"Hash#\"] % numPartitions)\n",
    "df.show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 3\n",
      "Partitioner: None\n",
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(Country#1, 3)\n",
      "+- Scan ExistingRDD[Amount#0L,Country#1,ID#2L]\n",
      "Partition 0:\n",
      "Partition 1:\n",
      "Row 0:Row(Amount=12, Country='US', ID=2)\n",
      "Row 1:Row(Amount=13, Country='CN', ID=3)\n",
      "Row 2:Row(Amount=15, Country='US', ID=5)\n",
      "Row 3:Row(Amount=16, Country='CN', ID=6)\n",
      "Row 4:Row(Amount=18, Country='US', ID=8)\n",
      "Row 5:Row(Amount=19, Country='CN', ID=9)\n",
      "Row 6:Row(Amount=21, Country='US', ID=11)\n",
      "Row 7:Row(Amount=22, Country='CN', ID=12)\n",
      "Partition 2:\n",
      "Row 8:Row(Amount=11, Country='AU', ID=1)\n",
      "Row 9:Row(Amount=14, Country='AU', ID=4)\n",
      "Row 10:Row(Amount=17, Country='AU', ID=7)\n",
      "Row 11:Row(Amount=20, Country='AU', ID=10)\n"
     ]
    }
   ],
   "source": [
    "print_partitions(df)\n",
    "\n",
    "df.write.mode(\"overwrite\").partitionBy(\"Country\").csv(\"data/example2.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this customised partitioning function, we guarantee each different country code gets a unique deterministic hash number.\n",
    "\n",
    "Now if we change the number of partitions to 2, both US and CN records will be allocated to one partition because:\n",
    "\n",
    "- CN (Hash# = 0): 0%2 = 0\n",
    "- US (Hash# = 2): 2%2 = 0\n",
    "- AU (Hash# = 1): 1%2 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "df = df.repartition(\"Country\")\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OTHER EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [\n",
    "    {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'},\n",
    "    {'name': 'James', 'amount': 15, 'country': 'United Kingdom'},\n",
    "    {'name': 'Marek', 'amount': 51, 'country': 'Poland'},\n",
    "    {'name': 'Johannes', 'amount': 200, 'country': 'Germany'},\n",
    "    {'name': 'Paul', 'amount': 75, 'country': 'Poland'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------+\n",
      "|amount|       country|    name|\n",
      "+------+--------------+--------+\n",
      "|   100|United Kingdom|     Bob|\n",
      "|    15|United Kingdom|   James|\n",
      "|    51|        Poland|   Marek|\n",
      "|   200|       Germany|Johannes|\n",
      "|    75|        Poland|    Paul|\n",
      "+------+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext \\\n",
    "        .parallelize(transactions) \\\n",
    "        .map(lambda x: Row(**x))\n",
    "    \n",
    "df = spark.createDataFrame(rdd)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 8\n",
      "Partitioner: None\n",
      "Partitions structure: [[], [Row(amount=100, country='United Kingdom', name='Bob')], [], [Row(amount=15, country='United Kingdom', name='James')], [Row(amount=51, country='Poland', name='Marek')], [], [Row(amount=200, country='Germany', name='Johannes')], [Row(amount=75, country='Poland', name='Paul')]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions: {}\".format(df.rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(df.rdd.glom().collect()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After 'repartition()'\n",
      "Number of partitions: 200\n",
      "Partitioner: None\n",
      "Partitions structure: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(amount=200, country='Germany', name='Johannes')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(amount=51, country='Poland', name='Marek'), Row(amount=75, country='Poland', name='Paul')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James')], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Repartition by column\n",
    "df2 = df.repartition(\"country\")\n",
    "    \n",
    "print(\"\\nAfter 'repartition()'\")\n",
    "print(\"Number of partitions: {}\".format(df2.rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(df2.rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(df2.rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "- When your are deciding the number of partitions to use, you need to take access paths into consideration, for example, are your partition keys commonly used in filters?\n",
    "\n",
    "- But keep in mind that partitioning will not be helpful in all applications. For example, if a given data frame is used only once, there is no point in partitioning it in advance. It’s useful only when a dataset is reused multiple times (in key-oriented situations using functions like join()).\n",
    "\n",
    "- Operations that benefit from partitioning\n",
    "All operations performing shuffling data by key will benefit from partitioning. Some examples are cogroup(), groupWith(), join(), leftOuterJoin(), rightOuterJoin(), groupByKey(), reduceByKey(), combineByKey() or lookup()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Partitioning in Spark (PySpark) In-depth Walkthrough https://kontext.tech/column/spark/296/data-partitioning-in-spark-pyspark-in-depth-walkthrough\n",
    "- Data Partitioning Functions in Spark (PySpark) Deep Dive. https://kontext.tech/column/spark/299/data-partitioning-functions-in-spark-pyspark-explained\n",
    "- http://ethen8181.github.io/machine-learning/big_data/spark_partitions.html\n",
    "- https://towardsdatascience.com/the-art-of-joining-in-spark-dcbd33d693c\n",
    "- Six Spark Exercises to Rule Them All: https://towardsdatascience.com/six-spark-exercises-to-rule-them-all-242445b24565"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Managing \"Exploding\" Big Data https://engineering.linkedin.com/blog/2017/06/managing--exploding--big-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
